"""
CLIP ê±´ì„¤ ë„ë©”ì¸ Fine-tuning
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import clip
import pandas as pd
from PIL import Image
from pathlib import Path
import logging
from typing import List, Tuple

logger = logging.getLogger(__name__)


class BuildingDamageDataset(Dataset):
    """ê±´ë¬¼ í”¼í•´ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹"""

    def __init__(
        self,
        images_dir="train/datasets/learning_data/learning_pictures",
        labels_file="train/datasets/learning_data/learning_texts.xlsx",
        clip_model=None,
    ):
        self.images_dir = Path(images_dir)
        self.clip_model = clip_model

        # ë¼ë²¨ ë°ì´í„° ë¡œë“œ
        self.df = pd.read_excel(labels_file)

        # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡
        valid_extensions = {".jpg", ".jpeg", ".png", ".webp", ".avif"}
        self.image_files = [
            f for f in self.images_dir.glob("*") if f.suffix.lower() in valid_extensions
        ]

        # í”¼í•´ ìœ í˜• ë§¤í•‘
        self.damage_descriptions = self._create_damage_descriptions()

    def _create_damage_descriptions(self) -> List[str]:
        """í”¼í•´ ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„±"""
        descriptions = []

        # learning_texts.xlsxì˜ í”¼í•´í˜„í™©, í”¼í•´ë¶€ìœ„ ì»¬ëŸ¼ í™œìš©
        for _, row in self.df.iterrows():
            try:
                part = str(row.get("í”¼í•´ ë¶€ìœ„", "ê±´ë¬¼"))
                status = str(row.get("í”¼í•´í˜„í™©", "ì†ìƒ"))
                description = f"{part}ì— {status} í”¼í•´"
                descriptions.append(description)
            except:
                descriptions.append("ê±´ë¬¼ í”¼í•´")

        # ê¸°ë³¸ í”¼í•´ ì„¤ëª…ë“¤ë„ ì¶”ê°€
        base_descriptions = [
            "ê±´ë¬¼ ì™¸ë²½ ê· ì—´ í”¼í•´",
            "ì§€ë¶• ëˆ„ìˆ˜ í”¼í•´",
            "ì°½ë¬¸ íŒŒì† í”¼í•´",
            "ì½˜í¬ë¦¬íŠ¸ ë°•ë¦¬ í”¼í•´",
            "ì² ê·¼ ë…¸ì¶œ í”¼í•´",
            "í™”ì¬ ì†ìƒ í”¼í•´",
            "êµ¬ì¡°ì  ë³€í˜• í”¼í•´",
            "ì •ìƒì ì¸ ê±´ë¬¼",
        ]
        descriptions.extend(base_descriptions)

        return list(set(descriptions))  # ì¤‘ë³µ ì œê±°

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_file = self.image_files[idx]

        try:
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
            image = Image.open(image_file)
            if image.mode != "RGB":
                image = image.convert("RGB")

            # CLIP ì „ì²˜ë¦¬
            if self.clip_model:
                image_input = self.clip_model[1](image).unsqueeze(0)
            else:
                image_input = image

            # ëœë¤í•˜ê²Œ í”¼í•´ ì„¤ëª… ì„ íƒ (ì‹¤ì œë¡œëŠ” ì´ë¯¸ì§€ì™€ ë§¤ì¹­ë˜ì–´ì•¼ í•¨)
            description_idx = idx % len(self.damage_descriptions)
            description = self.damage_descriptions[description_idx]

            return image_input.squeeze(0), description

        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜ {image_file}: {e}")
            # í´ë°±: ê¸°ë³¸ ë°ì´í„° ë°˜í™˜
            default_image = torch.zeros(3, 224, 224)
            return default_image, "ê±´ë¬¼ í”¼í•´"


class CLIPFineTuner:
    """CLIP ëª¨ë¸ Fine-tuning"""

    def __init__(self, model_name="ViT-B/32", device="auto"):
        self.device = (
            "cuda" if torch.cuda.is_available() and device == "auto" else "cpu"
        )

        # CLIP ëª¨ë¸ ë¡œë“œ
        self.model, self.preprocess = clip.load(model_name, device=self.device)

        # í›ˆë ¨ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        for param in self.model.parameters():
            param.requires_grad = True

        logger.info(f"CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}, Device: {self.device}")

    def create_data_loader(self, batch_size=16, num_workers=2):
        """ë°ì´í„° ë¡œë” ìƒì„±"""
        dataset = BuildingDamageDataset(clip_model=(self.model, self.preprocess))

        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            collate_fn=self._collate_fn,
        )

        return dataloader

    def _collate_fn(self, batch):
        """ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬"""
        images, texts = zip(*batch)

        # ì´ë¯¸ì§€ ìŠ¤íƒ
        images = torch.stack(images)

        # í…ìŠ¤íŠ¸ í† í°í™”
        text_tokens = clip.tokenize(texts, truncate=True).to(self.device)

        return images.to(self.device), text_tokens

    def train(self, epochs=10, learning_rate=1e-5, batch_size=8):
        """CLIP Fine-tuning í›ˆë ¨"""
        logger.info("CLIP Fine-tuning ì‹œì‘...")

        # ë°ì´í„° ë¡œë”
        dataloader = self.create_data_loader(batch_size=batch_size)

        # ì˜µí‹°ë§ˆì´ì € (ì‘ì€ learning rate ì‚¬ìš©)
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)

        # ì†ì‹¤ í•¨ìˆ˜ (CLIP ì›ë³¸ê³¼ ë™ì¼)
        loss_fn = nn.CrossEntropyLoss()

        self.model.train()

        for epoch in range(epochs):
            total_loss = 0
            num_batches = 0

            for batch_idx, (images, text_tokens) in enumerate(dataloader):
                try:
                    # Forward pass
                    logits_per_image, logits_per_text = self.model(images, text_tokens)

                    # CLIP ì†ì‹¤ ê³„ì‚°
                    batch_size = images.shape[0]
                    labels = torch.arange(batch_size, device=self.device)

                    loss_img = loss_fn(logits_per_image, labels)
                    loss_text = loss_fn(logits_per_text, labels)
                    loss = (loss_img + loss_text) / 2

                    # Backward pass
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

                    total_loss += loss.item()
                    num_batches += 1

                    if batch_idx % 10 == 0:
                        logger.info(
                            f"Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}"
                        )

                except Exception as e:
                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue

            avg_loss = total_loss / max(num_batches, 1)
            logger.info(f"Epoch {epoch+1} ì™„ë£Œ, Average Loss: {avg_loss:.4f}")

        logger.info("CLIP Fine-tuning ì™„ë£Œ!")

    def save_model(self, save_path="train/models/clip_finetuned.pt"):
        """Fine-tuned ëª¨ë¸ ì €ì¥"""
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)

        # ëª¨ë¸ ìƒíƒœë§Œ ì €ì¥ (CLIP êµ¬ì¡°ëŠ” ìœ ì§€)
        torch.save(
            {
                "model_state_dict": self.model.state_dict(),
                "model_name": "ViT-B/32",
            },
            save_path,
        )

        logger.info(f"Fine-tuned CLIP ëª¨ë¸ ì €ì¥: {save_path}")

    def load_finetuned_model(self, load_path="train/models/clip_finetuned.pt"):
        """Fine-tuned ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(load_path, map_location=self.device)
        self.model.load_state_dict(checkpoint["model_state_dict"])
        logger.info(f"Fine-tuned CLIP ëª¨ë¸ ë¡œë“œ: {load_path}")


def train_clip_finetuning():
    """CLIP Fine-tuning ì‹¤í–‰"""

    # Fine-tuner ì´ˆê¸°í™”
    fine_tuner = CLIPFineTuner()

    # í›ˆë ¨ ì‹¤í–‰ (ì‘ì€ epochsë¡œ í…ŒìŠ¤íŠ¸)
    fine_tuner.train(epochs=5, learning_rate=1e-6, batch_size=4)

    # ëª¨ë¸ ì €ì¥
    fine_tuner.save_model()

    return fine_tuner


if __name__ == "__main__":
    import logging

    logging.basicConfig(level=logging.INFO)

    print("ğŸš€ CLIP ê±´ì„¤ ë„ë©”ì¸ Fine-tuning ì‹œì‘...")
    fine_tuner = train_clip_finetuning()
    print("âœ… Fine-tuning ì™„ë£Œ!")
